<!DOCTYPE html>
<html lang="en">
	<head>
        <meta charset="UTF-8">
        <meta name="viewport" content="width=device-width, initial-scale=1.0">
        <link rel="stylesheet" href="../css/style.css">
        <link href="https://fonts.googleapis.com/css?family=Noto+Serif" rel="stylesheet">
        <link rel="shortcut icon" type="image/png" href="../img/favicon.ico">

     <!--    mathjax.org - using latex in the html -->
		<script src="https://polyfill.io/v3/polyfill.min.js?features=es6"></script>
		<script id="MathJax-script" async src="https://cdn.jsdelivr.net/npm/mathjax@3/es5/tex-mml-chtml.js"></script>
        
        <title>Linear Regression</title>
	</head>
	<body>
		<header class="header">
			<h1 class="header__name">Jimmy Neville</h1>
			<a href="../index.html" class="header__link">Home</a>
			<a href="../index.html#SH1" class="header__link">About Me</a>
			<a href="../index.html#SH2" class="header__link">Tutorials</a>
			<a href="../index.html#SH3" class="header__link">Workout</a>
			<a href="../index.html#SH4" class="header__link">Articles</a>
			<a href="../index.html#SH5" class="header__link">Books</a>
			<div class="icons">
				<a href="https://www.facebook.com/jimmy.neville.733">
					<svg class="icons__facebook">
						<use xlink:href="../img/sprite.svg#icon-facebook"></use>
					</svg>
				</a>
				<a href="https://github.com/jneville0815">
					<svg class="icons__github">
						<use xlink:href="../img/sprite.svg#icon-github"></use>
					</svg>
				</a>
			</div>
		</header>
		<main>
			<div class="small-screen">
				<div class="navbar">
					<h1 style="float: left; transform: translateY(-2rem);">Jimmy Neville</h1>
					<div class="dropdown">
						<button class="dropbtn">Menu
							<i class="fa fa-caret-down"></i>
						</button>
						<div class="dropdown-content">
							<a href="../index.html#SH0">Home</a>
							<a href="../index.html#SH1">About Me</a>
							<a href="../index.html#SH2">Tutorials</a>
							<a href="../index.html#SH3">Workout</a>
							<a href="../index.html#SH4">Articles</a>
							<a href="../index.html#SH5">Books</a>
						</div>
					</div> 
				</div>
			</div>
			
			<div class="main-section">
				<div class="main-section__title">
					Linear Regression
				</div>
				<div class="main-section__text">
					<p><i>4/7/2020</i> - 


<!-- 					$$J(\theta_0, \theta_1) = \frac{1}{2m} \sum_{i=1}^{m} (h_\theta(x^i) - y^i)^2$$

					repeat until convergence:
					$$\theta_j := \theta_j - \alpha \frac{\partial}{\partial\theta_j} J(\theta_0, \theta_1)$$
					where j = 0,1

					\(...\) -->

<!-- 					Red line m: 77
					Green line m: 71.2846347607053
					Red line b: -50
					Green line b: -2204.030226700242 -->


					Linear regression is one of the most well-known machine learning algorithms. Most people interested in machine learning will study this algorithm first. In this tutorial we're going to take a look at the math involved in the algorithm while also learning some basic machine learning terminology. I'm writing this tutorial after completing the linear regression portion of <a href="https://www.coursera.org/learn/machine-learning" class="in-text-link">Andrew Ng's Machine Learning Course</a>.

					<h2>Unsupervised Learning</h2>

					<p>Machine learning algorithms are classified as either unsupervised learning algorithms, supervised learning algorithms or reinforcement learning algorithms. Linear regression is an unsupervised learning algorithm which means that the output of the model is known in advance. An example of an unsupervised learning algorithm is a dataset that contains a list of houses with the size of each house and the price of each house. We know in advance that given the size of a house we want to predict the price. Supervised learning algorithms and reinforcement learning algorithms will be explained in a later tutorial. Below we have an example housing dataset with 5 entries:  </p>

					<img src="../img/lr-0.png" alt="dataframe" class="lr-0">

					<h2>Regression Problem</h2>

					<p>Unsupervised learning algorithms are broken down into regression problems and classification problems. Regression problems have continuous valued output, and classification problems have discrete valued output. Examples of continuous valued output are prices and temperatures, and examples of discrete valued output are true/false values and anything else that can be split up into individual categories.</p>

					<h2>Visualizing Our Data</h2>

					<p>To get a better understanding of the dataset we're working with it helps to visualize the data. Note that sometimes you will hear input values (x's) called "independent variables," and the output value (y) called a "dependent variable." For this example we only have one independent variable. Here we plot the price on the y-axis and the size on the x-axis. </p>

					<img src="../img/lr-1.png" alt="graph" class="lr-1">

					<p>Our goal with linear regression is to find a regression line that best fits our data. In the next plot you can see a green regression line and a red regression line; the green line fits our data best because the cost function is minimized when using the green line.  </p>

					<img src="../img/lr-2.png" alt="graph" class="lr-2">

					<h2>The Cost Function</h2>

					<p>Our linear regression function is defined as \(h_\theta(x)\) where $$h_\theta(x) = \theta_0 + \theta_1x$$ You can see that this function is the same \(y = m(x) + b\) function that you learned in high school. As a side note, another name for our linear regression function is "hypothesis", and I'll be refering to it as such throughout the rest of the tutorial. Another side note: when you see anything to the i'th power in this tutorial, the notation is not to be interpreted as an exponent; it's referring to the current iteration.</p>

					<p>The cost function is defined as $$J(\theta_0, \theta_1) = \frac{1}{2m} \sum_{i=0}^{m-1} (h_\theta(x^i) - y^i)^2$$ where \(m = \) the number of data points (which is 5 in our ongoing example). We want to find values of theta that will minimize the cost function. Before calculating \(J(\theta_0, \theta_1\)) for our red line, we need to visualize and compute the values for \(h_\theta(x^i\)). Our values for \(\theta_0\) and \(\theta_1\) are -50 and 77, respectively. Don't worry about how we got these values; in the next part of this tutorial we'll see how these values are found using gradient descent. We'll now look at the values located on the red line that are on the same vertical line as our data points. Each black point located on the red line in the plot below will be a value of \(h_\theta(x^i)\) when we compute the cost function. The black double-sided arrows in the plot below are the lengths that we're summing the squares of when we compute the cost function.</p>

					 <img src="../img/lr-3.png" alt="graph" class="lr-3">

				 	<p>These will be the values for \(h_\theta(x)\) when we compute the cost function: $$h_\theta(x^0) = -50 + 77(1250) = 96,200$$ $$h_\theta(x^1) = -50 + 77(1300) = 100,050$$ $$h_\theta(x^2) = -50 + 77(1500) = 115,450$$ $$h_\theta(x^3) = -50 + 77(1750) = 134,700$$ $$h_\theta(x^4) = -50 + 77(2000) = 153,950$$</p>

					<p>Plugging these values into the cost function we get: $$J(-50, 77) = \frac{1}{10} * $$ $$((96,200 - 90,000)^2 + $$ $$(100,050 - 95,000)^2 + $$ $$(115,450 - 100,000)^2 + $$ $$(134,700 - 110,000)^2 + $$ $$(153,950 - 150,000)^2)$$</p>

					<p>which simplifies to $$J(-50, 77) = \frac{1}{10} * $$ $$((6,200)^2 + $$ $$(5,050)^2 + $$ $$(15,450)^2 + $$ $$(24,700)^2 + $$ $$(3,950)^2)$$</p>

					<p>and for our final answer we get $$J(-50, 77) = 92,833,750$$</p>

					<p>If we were to repeat the same process for the green line, our final answer would be $$J(-2204.03, 71.28) = 30,264,510.62$$</p>

					<p>30,264,510.62 is the absolute lowest value we can get from the cost function. In other words, any combination of \(\theta_0\) and \(\theta_1\) values that we plug into the equation will not give us a value less than 30,264,510.62. Now let's take a look at how we find our \(\theta_0\) and \(\theta_1\) values using gradient descent.</p>

					<h2>Gradient Descent</h2>

					<p>The general algorithm for gradient descent is defined as $$\theta_j := \theta_j - \alpha \frac{\partial}{\partial\theta_j} J(\theta_0, \theta_1)$$</p>

					<p>Note that the ":=" operator is the assignment operator, and \(\alpha\) is a constant called the learning rate. The learning rate controls how large each step of gradient descent will be. Also note that \(\frac{\partial}{\partial\theta_j} J(\theta_0, \theta_1)\) represents taking the partial derivative of the cost function. I will not go over the calculus involved in taking a partial derivative in this tutorial, but there are many <a href="https://www.youtube.com/results?search_query=partial+derivatives" class="in-text-link">videos</a> on the subject if you're interested.</p>

					<p>In this equation \(j = 0,1\) represents the feature index number, and this algorithm will be repeated until convergence.</p>

					<p>Basically what happens here is the values of \(\theta_0\) and \(\theta_1\) keep getting updated until the cost function is minimized. This algorithm can be used for many machine learning applications, but in this tutorial we'll look at how it works with linear regression.</p>

					<p>After taking the partial derivative of \(\theta_0\) and \(\theta_1\), the gradient descent algorithm is defined as $$\theta_0 := \theta_0 - \alpha \frac{1}{m} \sum_{i=0}^{m-1} (h_\theta(x^i) - y^i)$$ $$\theta_1 := \theta_1 - \alpha \frac{1}{m} \sum_{i=0}^{m-1} (h_\theta(x^i) - y^i) * x^i$$</p>

					<p>This algorithm is specific to linear regression and will also be repeated until convergence.</p>

					<hr>
					<br>

					<a href="../index.html" class="in-text-link top">Back to homepage</a>
				</div>
			</div>
			<div class="footer">
				Contact Me: Jneville0815@yahoo.com
			</div>
		</main>
	</body>
</html>